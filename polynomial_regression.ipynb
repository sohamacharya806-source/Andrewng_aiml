{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6dacf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/housing/housing.csv\"\n",
    "data = pd.read_csv(url)\n",
    "data = data.dropna()\n",
    "\n",
    "X = data['median_income'].values.reshape(-1, 1)\n",
    "y = data['median_house_value'].values.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e618ed",
   "metadata": {},
   "source": [
    "taking the input as it is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c3eb6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(X)\n",
    "split = int(0.8 * m)\n",
    "\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f30477",
   "metadata": {},
   "source": [
    "This is known as train test split.\n",
    "Train-test split is crucial in machine learning to prevent overfitting, providing an unbiased evaluation of a model's real-world performance by testing it on unseen data, ensuring it learns general patterns rather than memorizing noise, and helping select the best model for new situations. It splits data into training (for learning) and testing (for evaluation) sets, mimicking deployment and assessing true predictive power. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53ac1364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_features(X, degree):\n",
    "    X_poly = X\n",
    "    for d in range(2, degree + 1):\n",
    "        X_poly = np.c_[X_poly, X ** d]\n",
    "    return X_poly\n",
    "degree = 3\n",
    "X_train_poly = polynomial_features(X_train, degree)\n",
    "X_test_poly = polynomial_features(X_test, degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd8cefc",
   "metadata": {},
   "source": [
    "making a polynomial feature and testing it and below it is feature scaling and adding a bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d6110a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = X_train_poly.mean(axis=0)\n",
    "std = X_train_poly.std(axis=0)\n",
    "\n",
    "X_train_poly = (X_train_poly - mean) / std\n",
    "X_test_poly = (X_test_poly - mean) / std\n",
    "\n",
    "X_train_poly = np.c_[np.ones((len(X_train_poly), 1)), X_train_poly]\n",
    "X_test_poly = np.c_[np.ones((len(X_test_poly), 1)), X_test_poly]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08b91614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_polynomial_regression(X, y, lr=0.01, epochs=3000, lambda_=0.1):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros((n, 1))\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        y_pred = X @ theta\n",
    "        error = y_pred - y\n",
    "\n",
    "        gradients = (2/m) * X.T @ error\n",
    "        gradients[1:] += 2 * lambda_ * theta[1:]  # bias term is already added in the first term and second term does not contain the bias term\n",
    "\n",
    "        theta -= lr * gradients\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441fca36",
   "metadata": {},
   "source": [
    "Gradient descent with l2 regularization after that we would require to tarin the model as polynomials may require testing after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17b743f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = train_polynomial_regression(\n",
    "    X_train_poly, y_train,\n",
    "    lr=0.01,\n",
    "    epochs=3000,\n",
    "    lambda_=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e64c1d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 84276.96953270337\n",
      "Test RMSE: 83599.99492906051\n"
     ]
    }
   ],
   "source": [
    "def rmse(X, y, theta):\n",
    "    return np.sqrt(np.mean((X @ theta - y) ** 2))\n",
    "\n",
    "train_rmse = rmse(X_train_poly, y_train, theta)\n",
    "test_rmse = rmse(X_test_poly, y_test, theta)\n",
    "\n",
    "print(\"Train RMSE:\", train_rmse)\n",
    "print(\"Test RMSE:\", test_rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f16eb0",
   "metadata": {},
   "source": [
    "RMSE evaluation and after that extra plots to visualize the polynomial curves"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
